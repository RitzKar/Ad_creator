{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIqPLl3jGgP4ghpFt1RWB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RitzKar/Ad_creator/blob/main/Finetuning_Tech16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine tuning"
      ],
      "metadata": {
        "id": "uZsSz6590Cuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8bfKjexz_oj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q -U bitsandbytes\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.38.1"
      ],
      "metadata": {
        "id": "jVXtoY9J0PlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate\n",
        "# !pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "k2xvcB930WXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
        "\n",
        "model_id = \"google/gemma-7b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=os.environ['HF_TOKEN'])"
      ],
      "metadata": {
        "id": "Vg8KEt970dZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: Greed, for lack of a better word,\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "UnSfn6HO0klJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "8H1wcDkI0oml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# Create an instance of the LoraConfig class with specific configurations.\n",
        "lora_config = LoraConfig(\n",
        "    # 'r' parameter: This might specify a rate, resolution, or reduction dimension,\n",
        "    # depending on the context within the model or system it's being applied to.\n",
        "    # The exact meaning of 'r=8' depends on the internal implementation of LoraConfig.\n",
        "    r=8,\n",
        "\n",
        "    # 'target_modules' parameter: Lists the components or modules of the model that\n",
        "    # the configuration will target. These could refer to different parts of a neural network,\n",
        "    # especially in the context of an attention mechanism or similar structure, where:\n",
        "    # - 'q_proj' could refer to the projection for queries,\n",
        "    # - 'o_proj' for outputs,\n",
        "    # - 'k_proj' for keys,\n",
        "    # - 'v_proj' for values,\n",
        "    # - 'gate_proj', 'up_proj', 'down_proj' could be custom projections or mechanisms\n",
        "    # within the model, possibly related to gating or hierarchical processing.\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "\n",
        "    # 'task_type' parameter: Specifies the kind of task the model or system is intended for.\n",
        "    # 'CAUSAL_LM' indicates a Causal Language Model, a type of model used for generating text\n",
        "    # where each output token is predicted based on the preceding ones, without looking ahead\n",
        "    # to future tokens. This is common in generative tasks where understanding the sequence\n",
        "    # order is crucial.\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "EsHVa_lH0shH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "id": "sCWR1zJ90ybY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 3 examples\n",
        "for i in range(3):\n",
        "    print(data['train'][i])"
      ],
      "metadata": {
        "id": "l8eo0vmr021D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def formatting_func(example):\n",
        "    text = f\"Quote: {example['quote'][0]}\\nAuthor: {example['author'][0]}\"\n",
        "    return [text]\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1i6JaZpP07Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: Greed, for lack of a better word,  \"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=30)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "_pJJm4Iu0_ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gemma"
      ],
      "metadata": {
        "id": "tJMfzgFk1Kod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate\n",
        "# !pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "hgTdSutF1Fs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "FkcUnzQe1R2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Summarize the following text in 3 bullet points:\n",
        "\n",
        "\n",
        "\tOpec+ members led by Saudi Arabia and Russia have extended voluntary cuts to oil production for another three months, as they attempt to boost prices that have remained subdued in spite of geopolitical tensions.\n",
        "\n",
        "The curbs were due to expire at the end of March but will continue until the end of June, according to Saudi Arabia’s state news agency.\n",
        "\n",
        "The measures add to a series of output cuts by Opec+ members since 2022 designed to support prices amid rising US production and tepid global demand. Since the latest voluntary cuts came into effect in January they have lowered the combined production targets of members by about 2.2mn barrels a day.\n",
        "\n",
        "“The decision sends a message of cohesion and confirms that the group is not in a hurry to return supply volumes, supporting the view that when this finally happens, it will be gradual,” said Giacomo Romeo, an analyst at Jefferies.\n",
        "\n",
        "Brent crude, the international benchmark, has risen by 6 per cent and the US equivalent WTI almost 8 per cent since the latest cuts were first announced at the end of November.\n",
        "\n",
        "But despite tensions in the Middle East, including the Israel-Hamas war and the attacks on commercial shipping by the Houthis, the oil price remains well below the $100 a barrel level last seen in the summer of 2022.\n",
        "\n",
        "Traders had largely expected the decision to extend the curbs, with crude oil prices rising last week ahead of the announcement. Brent rose more than 2 per cent last week to close above $83 a barrel on Friday, while WTI closed just under $80 a barrel, a rise of more than 4 per cent.\n",
        "\n",
        "Opec+ was “trying to keep the market in balance”, said Amrita Sen at Energy Aspects. “Oil prices are a lot more stable . . . but they want to ensure the stability continues,” she said.\n",
        "\n",
        "Saudi Arabia has shouldered most of the curbs, having cut its production by 1mn b/d since July. In total, the kingdom is producing 2mn b/d a less than it did in October 2022. In January, it dropped its plans to expand its daily oil production capacity by 2027 in a major policy reversal.\n",
        "\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**input_ids, max_length=1000)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "z2m9K30Z1U0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q&A function\n",
        "\n",
        "def answer_the_question(question):\n",
        "    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids\n",
        "    generated_text = model.generate(\n",
        "        input_ids.to(\"cuda\"),\n",
        "        max_length=1000,\n",
        "    )\n",
        "    answer = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "question = \"Write a short rap about stanford students taking an LLM class.\"\n",
        "answer = answer_the_question(question)\n",
        "print(f\"Question: {question}\\nAnswer: {answer}\")"
      ],
      "metadata": {
        "id": "4hgs64rM1Zbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzXlCKAk1dxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}